\documentclass[11pt,twoside]{article}
\usepackage{asp2014}

\aspSuppressVolSlug
\resetcounters

\bibliographystyle{asp2014}

\markboth{Mueller et al.}{Qserv Distributed Petascale Database}

\begin{document}
%\ssindex{LSST}
%\ssindex{databases!petascale}
%\ssindex{observatories!ground-based!LSST}
%\ssindex{observatories!ground-based!Rubin}

\title{Qserv: A Distributed Petascale Database for the LSST Catalogs}

\input{authors}

\begin{abstract}
Qserv is a distributed, shared-nothing, SQL database system being developed by the Vera Rubin Observatory to
host the multi-petabyte astronomical catalogs that will be produced by the LSST survey.  Here we sketch the
basic design and operating principles of Qserv, and provide some updates on recent developments.
\end{abstract}

\section{Introduction}

The Legacy Survey of Space and Time \citep{2019ApJ...873..111I} is a "deep fast wide" optical/near-IR survey
of half the sky in \emph{ugrizy} bands to \emph{r} 27.5 (36\,nJy) based on 825 visits over a 10-year period,
to be carried out by the Vera C.\ Rubin Observatory in Chile.

The astronomical catalogs to be produced by the survey are notionally described in \citet{LSE-163}, and the
corresponding database schema is described in \citet{LDM-153}.  By the 10th year of the survey, the catalog
database is expected to run to approximately 60 trillion rows, requiring more than 10 petabytes of storage
before considerations for replication, indices, or other overheads.

At the outset of construction, the project could not identify a mature database product capable of operating
at these scales while also meeting project requirements for robust spherical geometry, predictable query
response under heavy concurrent load, platform/hardware affordability, and open source licensing.  The Rubin
Observatory thus embarked on the design and implementation of Qserv.

\section{The Qserv Approach}

The evolution and progress of Qserv has been previously reported in \citet{2006SPIE.6270E..0RB},
\citet{2011Wang:2011:QDS:2063348.2063364}, and \citet{LDM-135}.  Principal design elements are as follows:

\subsection{Distributed and Parallel}

Qserv is designed as a parallel database, where catalog data is spatially partitioned and distributed
uniformly over a collection of shared-nothing back-end nodes.  Queries are handled in a "map/reduce" paradigm:
after arriving at a SQL proxy, queries are analyzed for spatial coverage and rewritten into a collection of
supporting partial queries which are then distributed to involved back-end nodes for execution ("map"). The
results of these partial queries are subsequently returned to the proxy node where they are aggregated and any
necessary summary statistics computed ("reduce"); results are then returned to the user.

\subsection{Partitioning and Replication}

Qserv divides data into spatial partitions of roughly the same area.  Since partitions are small with respect
to higher-density areas of the survey and spread over back-end using a non-area-based scheme,
density-differential-induced skew is averaged out among the nodes.

Qserv additionally stores a precomputed amount of overlapping data from spatially adjacent partitions
alongside each partition.  Using this data, spatial joins can be computed correctly within a preset
distance without needing data from other partitions that may be located on other nodes.

Data partitions are dynamically replicated across back-end nodes, for resilience and high-availability.  If a
node becomes unavailable, Qserv is capable of re-dispatching in-flight partial queries involving partitions on
that node toward other available partition replicas transparently to the user.  Partitions that become
under-represented will be re-replicated and the cluster re-balanced in the background.  The same facility
enables adding nodes to a Qserv cluster or draining nodes for maintenance as online operations.

\subsection{Shared Scans}

Shared scans are a key component of the Qserv architecture to enable predictable query throughput on fixed
hardware under heavily concurrent loads.  As partial queries arrive at worker nodes, they are held and grouped
by involved partition(s).  Each node scans its local partitions in a cyclic, coordinated fashion. When a scan
visits a partition, its table backing files are locked into memory and all batched queries involving that
partition are executed.  The tables are then released and the scan proceeds on to the next local partition.

Multiple scan queues per node are supported, to accommodate scans that execute at significantly different
speeds due to the different amounts of I/O required.  In this way, time-consuming I/O intensive scans need not
hold up scans of other tables that could otherwise proceed more quickly.  Some capacity at each node is also
reserved to be able to quickly answer ad-hoc, non-scan queries to partitions on that node.

Partial queries may join a scan queue in any phase as they arrive at a node, and remain on the scan until all
involved partitions on that node are visited.

\subsection{Designed for Purpose}

Qserv, while broadly general purpose, has been specifically designed to meet the LSST catalog use case.  This
has allowed leverage of some significant simplifications to keep complexity low and to keep the project
feasible for a small development team.  For example: the LSST catalogs are to be released annually and are a
read-only product; thus Qserv did not need to address SQL UPDATE semantics nor encompass a full transactional
model.

The astronomical use-case demands robust spherical geometry within the database.  Qserv has addressed
this by providing a library of spherical geometry UDFs which are made available for use of partial queries on
the back-end worker nodes.

Qserv has leveraged mature, open source, off-the-shelf components wherever possible; these include MariaDB,
the XRootD distributed file system, and Google protobufs to name a few.

Qserv is fully containerized, and has a Kubernetes operator to facilitate deployment on Kubernetes either
on-prem or in the commodity cloud.  We have found about a 15\% performance impact when running with network
storage on the Google cloud compared to running with locally attached storage on an on-prem cluster.

\section{Performance Targets and Sizing}

Performance targets for Qserv in the context of LSST have been established as follows.  Under a combined,
sustained load of 100 concurrent small ad-hoc queries and 50 concurrent full scans:
\[
{\renewcommand{\arraystretch}{1.5}\begin{array}{ll}
  \parbox{9cm}{Retrieve common attributes of an object, or all objects within a small area of the sky}
    & \mbox{< 10 seconds} \\
  \mbox{Scan entire sky (billions of objects)}
    & \mbox{$\approx{}$1 hour} \\
  \mbox{Deeper analysis (incl. object extended attribute BLOBs)}
    & \mbox{$\approx{}$8 hours} \\
  \mbox{Detection or forced photometry full scans}
    & \mbox{$\approx{}$12 hours} \\
\end{array}}
\]

Since Qserv is principally I/O limited, a cluster is sized according to target full-scan times, using the
following simple relations:

\[
    \frac{\text{table size(s)}}{\text{sustained I/O rate} * \text{workers}} = \text{target scan time}
\]

\[
    \frac{(\text{table size(s)} + \text{overlap} + \text{local indices}) * \text{replication}}
    {\text{workers}} = \text{local worker storage} \\
\]

For LSST, with consideration of the latest data sizing models \citep{DMTN-135}, this works out to a cluster of
approximately 100 worker nodes for year 1 of the survey, peaking at approximately 450 worker nodes in year 6,
with approximately 45 TiB of local storage attached to each node.

\acknowledgements This material or work is supported in part by the National Science Foundation through
Cooperative Agreement AST-1258333 and Cooperative Support Agreement AST1836783 managed by the Association of
Universities for Research in Astronomy (AURA), and the Department of Energy under Contract No.
DE-AC02-76SF00515 with the SLAC National Accelerator Laboratory managed by Stanford University.

\bibliography{C15}

\end{document}
